{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写程序从网络中自动获取数据的过程叫做数据爬取，也叫网络爬虫，一般步骤为：\n",
    "1.获取爬取页面的url\n",
    "2.获取页面内容\n",
    "3.解析页面\n",
    "4.获取所需数据、\n",
    "\n",
    "明星图片爬取基于百度搜索的返回结果进行，在百度搜索“中国艺人”，解析返回页面中展示的艺人图片链接并保存"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤1：定义爬取指定url页面的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\redamancy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.28.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\redamancy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redamancy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\redamancy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redamancy\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1.0页下载完成\n",
      "第2.0页下载完成\n",
      "第3.0页下载完成\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import urllib\n",
    "# 直接使用程序爬取网络数据会被网站识别出来，然后封禁该IP，导致数据爬\n",
    "# 取中断，所以我们需要首先将程序访问页面伪装成浏览器访问页面\n",
    "# User-Agent：定义一个真实浏览器的代理名称，表明自己的身份（是哪种浏览器），本demo为谷歌浏览器\n",
    "# Accept：告诉WEB服务器自己接受什么介质类型，*/* 表示任何类型\n",
    "# Referer：浏览器向WEB服务器表明自己是从哪个网页URL获得点击当前请求中的网址/URL\n",
    "# Connection：表示是否需要持久连接\n",
    "# Accept-Language：浏览器申明自己接收的语言\n",
    "# Accept-Encoding：浏览器申明自己接收的编码方法，通常指定压缩方法，是\n",
    "# 否支持压缩，支持什么压缩方法（gzip，deflate）\n",
    "class GetImage():\n",
    "    def __init__(self,keyword='大雁',paginator=1):\n",
    "        # self.url: 链接头\n",
    "        self.url = 'http://image.baidu.com/search/acjson?'\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT\\\n",
    "             10.0; WOW64) AppleWebKit/537.36\\\n",
    "              (KHTML, like Gecko) Chrome/69.0.\\\n",
    "            3497.81 Safari/537.36'}\n",
    "        self.headers_image = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows\\\n",
    "             NT 10.0; WOW64) AppleWebKit/537.36 \\\n",
    "             (KHTML, like Gecko) Chrome/69.0.\\\n",
    "            3497.81 Safari/537.36',\n",
    "            'Referer': 'http://image.baidu.com/\\\n",
    "            search/index?tn=baiduimage&ipn=r&\\\n",
    "            ct=201326592&cl=2&lm=-1&st=-1&\\\n",
    "            fm=result&fr=&sf=1&fmq=1557124645631_R&\\\n",
    "            pv=&ic=&nc=1&z=&hd=1&latest=0&copyright\\\n",
    "            =0&se=1&showtab=0&fb=0&width=&height=\\\n",
    "            &face=0&istype=2&ie=utf-8&sid=&word=%\\\n",
    "            E8%83%A1%E6%AD%8C'}\n",
    "        self.keyword = keyword      # 定义关键词\n",
    "        self.paginator = paginator  # 定义要爬取的页数\n",
    "\n",
    "    def get_param(self):\n",
    "        # 将中文关键词转换为符合规则的编码\n",
    "        keyword = urllib.parse.quote(self.keyword)\n",
    "        params = []\n",
    "        # 为爬取的每页链接定制参数\n",
    "        for i in range(1, self.paginator + 1):\n",
    "            params.append(\n",
    "                'tn=resultjson_com&ipn=rj&ct=201326592&is=&\\\n",
    "                fp=result&queryWord={}&cl=2&lm=-1&ie=utf-8&o\\\n",
    "                e=utf-8&adpicid=&st=-1&z=&ic=&hd=1&latest=0&\\\n",
    "                copyright=0&word={}&s=&se=&tab=&width=&height\\\n",
    "                =&face=0&istype=2&qc=&nc=1&fr=&expermode=&for\\\n",
    "                ce=&cg=star&pn={}&rn=30&gsm=78&1557125391211\\\n",
    "                ='.format(keyword, keyword, 30 * i))\n",
    "        return params   # 返回链接参数\n",
    "\n",
    "    def get_urls(self, params):\n",
    "        urls = []\n",
    "        for param in params:\n",
    "            # 拼接每页的链接\n",
    "            urls.append(self.url + param)\n",
    "        return urls   # 返回每页链接\n",
    "\n",
    "    def get_image_url(self, urls):\n",
    "        image_url = []\n",
    "        for url in urls:\n",
    "            json_data = requests.get(url, headers=self.headers).json()\n",
    "            json_data = json_data.get('data')\n",
    "            for i in json_data:\n",
    "                if i:\n",
    "                    image_url.append(i.get('thumbURL'))\n",
    "        return image_url\n",
    "\n",
    "    def get_image(self, image_url):\n",
    "        \"\"\"\n",
    "        根据图片url，在本地目录下新建一个以搜索关键字命名的文件夹，然后将每一个图片存入。\n",
    "        :param image_url:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        cwd = os.getcwd()\n",
    "        file_name = os.path.join(cwd, self.keyword)\n",
    "        if not os.path.exists(self.keyword):\n",
    "            os.mkdir(file_name)\n",
    "        for index, url in enumerate(image_url, start=1):\n",
    "            with open(file_name+'/{}_0.jpg'.format(index), 'wb') as f:\n",
    "                f.write(requests.get(url, headers=self.headers_image).content)\n",
    "            if index != 0 and index % 30 == 0:\n",
    "                print('第{}页下载完成'.format(index/30))\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        params = self.get_param()  # 获取链接参数\n",
    "        urls = self.get_urls(params)\n",
    "        image_url = self.get_image_url(urls)\n",
    "        self.get_image(image_url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spider = GetImage('鱼', 3)\n",
    "    spider()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
